{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:16:27.476643Z",
     "start_time": "2023-11-19T15:16:27.442753Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer_uni = Tokenizer.from_file(\n",
    "    os.path.join('../dags/src/spellcheck/data/', 'tokenizer_unigram_5k.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('../data/tesstrain/kbd/configs/kbd.wordlist', 'r') as f:\n",
    "    words = f.read().split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:16:28.316149Z",
     "start_time": "2023-11-19T15:16:28.264954Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "data = []\n",
    "\n",
    "regexps = defaultdict(set)\n",
    "\n",
    "tokenized_words = set()\n",
    "\n",
    "for word in sorted(words):\n",
    "    tokens = tokenizer_uni.encode(word).tokens\n",
    "    token_ids = [tokenizer_uni.token_to_id(token) for token in tokens]\n",
    "    \n",
    "    for limit in [100, 200, 300, 500, 1000]:\n",
    "        filtered_tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if not token_id or token_id > limit:\n",
    "                filtered_tokens.append(f'*')\n",
    "            else:\n",
    "                filtered_tokens.append(tokenizer_uni.id_to_token(token_id))\n",
    "        \n",
    "        reg = '|'.join(filtered_tokens)\n",
    "        tokenized_word = '|'.join(tokens)\n",
    "        tokenized_word_ids = '|'.join([str(token_id) for token_id in token_ids])\n",
    "        \n",
    "        tokenized_words.add(tokenized_word)\n",
    "        regexps[reg].add((tokenized_word, tokenized_word_ids))\n",
    "        \n",
    "        \n",
    "tokenized_text = '\\n'.join(sorted(tokenized_words))\n",
    "with open('../data/tesstrain/kbd/tokenized_text.txt', 'w') as f:\n",
    "    f.write(tokenized_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:16:39.669448Z",
     "start_time": "2023-11-19T15:16:29.022676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "for word in tokenized_words:\n",
    "    cnt.update(nltk.ngrams(word.split('|'), 2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:16:41.020456Z",
     "start_time": "2023-11-19T15:16:39.670069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('къы', 'зэ', 'ры'), 4222),\n (('щ', 'хь', 'э'), 3560),\n (('лъ', 'агъ', 'у'), 2523),\n (('хэ', 'м', 'рэ'), 2003),\n (('тI', 'ы', 'с'), 1955),\n (('л', 'эж', 'ь'), 1789),\n (('щI', 'ы', 'хь'), 1752),\n (('п', 'лъ', 'э'), 1642),\n (('цI', 'ы', 'ху'), 1639),\n (('у', 'п', 'щI'), 1529),\n (('зэ', 'ры', 'зэ'), 1500),\n (('щI', 'э', 'у'), 1462),\n (('ы', 'с', 'хь'), 1403),\n (('хь', 'э', 'у'), 1390),\n (('къы', 'щI', 'э'), 1357),\n (('б', 'гъэ', 'дэ'), 1344),\n (('э', 'хэ', 'м'), 1326),\n (('гъэ', 'хь', 'э'), 1257),\n (('къ', 'и', 'гъэ'), 1236),\n (('т', 'хь', 'э'), 1209),\n (('ын', 'у', 'р'), 1196),\n (('гъэ', 'кIу', 'э'), 1189),\n (('гъэ', 'тI', 'ы'), 1167),\n (('эж', 'ын', 'у'), 1159),\n (('эн', 'у', 'р'), 1115),\n (('зэ', 'ры', 'щы'), 1108),\n (('зэ', 'щI', 'э'), 1081),\n (('лъ', 'ы', 'хь'), 1079),\n (('зэ', 'фI', 'э'), 1070),\n (('э', 'хэ', 'р'), 1063),\n (('кI', 'ын', 'у'), 1040),\n (('Iэ', 'щI', 'э'), 1033),\n (('лъ', 'хь', 'э'), 1031),\n (('хь', 'э', 'лI'), 1027),\n (('щI', 'э', 'кI'), 997),\n (('ху', 'и', 'гъэ'), 985),\n (('къы', 'с', 'ху'), 973),\n (('п', 'лъ', 'ы'), 965),\n (('къы', 'зэ', 'ри'), 954),\n (('ыж', 'ын', 'у'), 941),\n (('р', 'тэ', 'къым'), 934),\n (('п', 'хъ', 'э'), 929),\n (('э', 'кI', 'ын'), 925),\n (('хь', 'э', 'щы'), 918),\n (('хь', 'эн', 'у'), 875),\n (('зэ', 'ры', 'щI'), 869),\n (('п', 'щI', 'э'), 836),\n (('гъэ', 'у', 'вэ'), 835),\n (('кIу', 'э', 'т'), 828),\n (('кIэ', 'лъ', 'ы'), 821),\n (('п', 'са', 'лъ'), 816),\n (('зэ', 'ри', 'гъэ'), 803),\n (('щI', 'э', 'п'), 793),\n (('эн', 'ы', 'гъэ'), 788),\n (('ри', 'хь', 'э'), 775),\n (('ды', 'хь', 'э'), 769),\n (('хь', 'э', 'зы'), 762),\n (('лъ', 'ы', 'тэ'), 760),\n (('сы', 'зэ', 'ры'), 744),\n (('щI', 'ын', 'у'), 743),\n (('щI', 'а', 'гъэ'), 741),\n (('къы', 'с', 'хуэ'), 741),\n (('щI', 'эн', 'у'), 739),\n (('ху', 'и', 'щI'), 732),\n (('э', 'у', 'рэ'), 730),\n (('щI', 'и', 'гъэ'), 726),\n (('а', 'хэ', 'м'), 725),\n (('ын', 'у', 'къым'), 724),\n (('щI', 'э', 'лъ'), 722),\n (('и', 'ри', 'гъэ'), 720),\n (('лъ', 'ы', 'хъу'), 719),\n (('хь', 'э', 'щI'), 703),\n (('къ', 'а', 'гъэ'), 698),\n (('хь', 'ау', 'э'), 696),\n (('ды', 'зэ', 'ры'), 694),\n (('е', 'п', 'лъ'), 693),\n (('лъ', 'э', 'Iу'), 686),\n (('ат', 'э', 'къым'), 677),\n (('ны', 'къу', 'э'), 670),\n (('I', 'эн', 'у'), 668),\n (('лъ', 'хь', 'эж'), 667),\n (('е', 'з', 'гъэ'), 665),\n (('кIу', 'э', 'цI'), 660),\n (('лъ', 'э', 'кI'), 650),\n (('хь', 'э', 'ри'), 649),\n (('зэ', 'ры', 'т'), 643),\n (('лъ', 'э', 'щI'), 635),\n (('гъэ', 'лъ', 'эгъу'), 635),\n (('у', 'тI', 'ып'), 635),\n (('кIу', 'э', 'кI'), 634),\n (('зэ', 'ра', 'гъэ'), 632),\n (('кIу', 'э', 'ды'), 630),\n (('лъ', 'хь', 'эн'), 626),\n (('зэ', 'ры', 'хуэ'), 624),\n (('э', 'щI', 'э'), 620),\n (('псэ', 'лъ', 'ы'), 616),\n (('гу', 'ры', 'Iу'), 614),\n (('гъэ', 'лъ', 'э'), 612),\n (('гъэ', 'лъ', 'агъ'), 611),\n (('кIу', 'эн', 'у'), 607)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:16:41.047088Z",
     "start_time": "2023-11-19T15:16:41.032145Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "filtered_regexps = {\n",
    "    k: v for k, v in regexps.items() if k.count('*') <= 2 and 10 < len(v) < 20\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-19T15:00:24.342473Z",
     "start_time": "2023-11-19T15:00:24.273311Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "os.makedirs(f'../data/tesstrain/kbd/tokens/', exist_ok=True)\n",
    "for k, v in sorted(filtered_regexps.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    wildcards = k.count('*')\n",
    "    name = f'({wildcards}){k.replace(\"*\", \"_\")}({len(v)}).txt'\n",
    "    with open(f'../data/tesstrain/kbd/tokens/{name}', 'w') as f:\n",
    "        for tokens, token_ids in v:\n",
    "            f.write(f'{tokens}\\t{token_ids}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:00:26.518291Z",
     "start_time": "2023-11-19T15:00:26.379706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def build_regexp(key, tokens):\n",
    "    groups = key.split('|')  # Разделяем шаблон на группы\n",
    "    pattern_parts = []\n",
    "\n",
    "    token_split = [token.split('|') for token in tokens]\n",
    "    \n",
    "    for index, group in enumerate(groups):\n",
    "        if group == '*':\n",
    "            tokens = '|'.join([token[index] for token in token_split])\n",
    "            if tokens.count('|') < 15:\n",
    "                pattern_parts.append(f'(?P<g_{index}>{tokens})')  # Именованный захват для *\n",
    "            else:\n",
    "                pattern_parts.append(f'(?P<g_{index}>.+)')  # Именованный захват для *\n",
    "        else:\n",
    "            pattern_parts.append(f'(?P<gf_{index}>' + group + ')')  # Именованный захват для остальных групп\n",
    "    \n",
    "    pattern = '\\\\|'.join(pattern_parts)  # Собираем шаблон обратно\n",
    "    r = re.compile(pattern)\n",
    "    return r\n",
    "\n",
    "regexps_words = defaultdict(set)\n",
    "for k, v in sorted(filtered_regexps.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    tokens = [v[0] for v in v]\n",
    "    r = build_regexp(k, tokens)\n",
    "    for m in r.finditer(tokenized_text):\n",
    "        if m:\n",
    "            gd = m.groupdict()\n",
    "            extracted = ''.join([gd[f'g_{i}'] for i in range(len(gd)) if f'g_{i}' in gd])\n",
    "            regexps_words[r].add(k)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T15:03:27.444126Z",
     "start_time": "2023-11-19T15:00:32.896195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
