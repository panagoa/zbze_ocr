{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(BPE())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:07:21.280777Z",
     "start_time": "2023-11-12T02:07:21.263114Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "custom_tokens = [\n",
    "    'гу',\n",
    "    'гъ',\n",
    "    'гъу',\n",
    "    'дж',\n",
    "    'дз',\n",
    "    'жь',\n",
    "    'ку',\n",
    "    'кӀ',\n",
    "    'кIу',\n",
    "    'къ',\n",
    "    'къу',\n",
    "    'кхъ',\n",
    "    'кхъу',\n",
    "    'лъ',\n",
    "    'лI',\n",
    "    'пI',\n",
    "    'тI',\n",
    "    'фI',\n",
    "    'ху',\n",
    "    'хь',\n",
    "    'хъ',\n",
    "    'хъу',\n",
    "    'цI',\n",
    "    'щI',\n",
    "    'Ӏу',\n",
    "\n",
    "    'хуэ',\n",
    "]\n",
    "\n",
    "custom_tokens += [\n",
    "    'во',\n",
    "    'рэ',\n",
    "    'зэ',\n",
    "    'де',\n",
    "    'зо',\n",
    "    'ар',\n",
    "    'ди',\n",
    "    'ремы',\n",
    "    'эрэ',\n",
    "    'хуэ',\n",
    "    'дэ',\n",
    "    'ры',\n",
    "    'къ',\n",
    "    'эж',\n",
    "    'ри',\n",
    "    'ыж',\n",
    "    'фы',\n",
    "    'мы',\n",
    "    'ау',\n",
    "    'щы',\n",
    "    'къым',\n",
    "    'хуа',\n",
    "    'эм',\n",
    "    'ат',\n",
    "    'хэ',\n",
    "    'нщ',\n",
    "    'зы',\n",
    "    'жы',\n",
    "    'тэ',\n",
    "    'ро',\n",
    "    'ды',\n",
    "    'хы',\n",
    "    'къэ',\n",
    "    'рызы',\n",
    "    'ащ',\n",
    "    'ре',\n",
    "    'да',\n",
    "    'до',\n",
    "    'ам',\n",
    "    'гъэ',\n",
    "    'вэ',\n",
    "    'ми',\n",
    "    'къы',\n",
    "    'ху',\n",
    "    'ын',\n",
    "    'ха',\n",
    "    'эн',\n",
    "    'фӀэ',\n",
    "    'ущ',\n",
    "    'ра',\n",
    "    'къу'\n",
    "]\n",
    "\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "\n",
    "vocab_size = 3000\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "input_dir = os.path.join('..', 'data/tesstrain/kbd/data/input')\n",
    "output_dir = os.path.join('..', 'data/tesstrain/kbd/data/output')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-11-12T02:09:01.227527Z",
     "start_time": "2023-11-12T02:09:01.116652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "file_path = os.path.join(input_dir, 'oshamaho.txt')\n",
    "tokenizer.train([file_path], trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-11-12T02:15:10.358141Z",
     "start_time": "2023-11-12T02:09:04.335413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tokenizer.save(os.path.join(output_dir, f'vocab_{vocab_size}.json'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-11-12T01:29:30.249707Z",
     "start_time": "2023-11-12T01:29:30.226270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Использование токенизатора\n",
    "encoded = tokenizer.encode('ЦӀыхум и гум удыхьэн папщӀэ, абы и бгъэр зэгуэбгъэжын хуейкъым.')\n",
    "print(encoded.tokens)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
